import torch
import gc


def setup_gpu():
    """Ø¥Ø¹Ø¯Ø§Ø¯ GPU ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ¯ GPU Ù…ØªØ§Ø­: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ¯ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

        # ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

        return device
    else:
        print("âš ï¸  GPU ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU")
        return torch.device("cpu")


def clear_gpu_memory():
    """ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© GPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def get_gpu_memory_usage():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© GPU"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024 ** 3
        reserved = torch.cuda.memory_reserved() / 1024 ** 3
        return allocated, reserved
    return 0, 0


def print_gpu_memory_usage():
    """Ø·Ø¨Ø§Ø¹Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© GPU"""
    if torch.cuda.is_available():
        allocated, reserved = get_gpu_memory_usage()
        print(f"ğŸ’¾ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {allocated:.2f} GB / {reserved:.2f} GB Ù…Ø­Ø¬ÙˆØ²Ø©")
        
