import cv2
import numpy as np
import torch
from typing import Dict, Any, List
from collections import Counter
import logging
from model_loader import ModelLoader
from translation_utils import MarianTranslator # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
from PIL import Image

logger = logging.getLogger(__name__)

model_loader = ModelLoader()
translator = MarianTranslator() # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù‡Ù†Ø§


class ActivityRecognizer:
    # Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„ØªØ´ØºÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ VideoMAE (Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø©)
    FRAME_SEQUENCE_LENGTH = 16
    # Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø²Ù„Ù‚Ø© (Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ­Ø±ÙƒÙ‡Ø§ Ø§Ù„Ù†Ø§ÙØ°Ø© ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©)
    # Ù‚ÙŠÙ…Ø© 4 ØªØ¹Ù†ÙŠ ØªØ¯Ø§Ø®Ù„ 12 Ø¥Ø·Ø§Ø±Ù‹Ø§ (16 - 4)
    SLIDING_WINDOW_STRIDE = 8

    def __init__(self):
        try:
            # ØªØ­Ù…ÙŠÙ„ ÙƒÙ„Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ù…Ù† model_loader
            self.videomae_proc, self.videomae_model, self.blip_proc, self.blip_model = model_loader.load_scene_recognition_model()

            if self.videomae_model is None or self.blip_model is None:
                raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø£Ø­Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.")

            self.device = next(self.videomae_model.parameters()).device
            self.videomae_model.eval()
            self.blip_model.eval()

            # Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªÙƒÙˆÙŠÙ† ØªØ³Ù„Ø³Ù„
            self.frame_buffer = []
            # Ø¹Ø¯Ø§Ø¯ Ù„Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ø¶Ø§ÙØªÙ‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù…Ù†Ø° Ø¢Ø®Ø± ØªØ­Ù„ÙŠÙ„
            self.frames_since_last_analysis = 0

            print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù†Ù…ÙˆØ°Ø¬ÙŠ VideoMAE Ùˆ BLIP")
        except Exception as e:
            self.videomae_model = self.blip_model = None
            self.device = None
            print(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer. ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

        # ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ
        self.scene_history = []
        self.max_history = 50

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø§Ø·
        self.activity_stats = {
            "total_frames_processed": 0, # Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ Ù…Ø±Øª Ø¹Ù„Ù‰ recognize_activity
            "successful_detections": 0,  # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø§Ø¬Ø­ (Ù†Ø§ÙØ°Ø© ÙƒØ§Ù…Ù„Ø©)
            "failed_detections": 0       # Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Ù†Ø§ÙØ°Ø© ÙƒØ§Ù…Ù„Ø©)
        }

    def _analyze_blip(self, frame_pil: Image.Image) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ù…ÙˆØ°Ø¬ BLIP Ù„ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù†ØµÙŠ."""
        if self.blip_model is None:
            return {"description": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­", "confidence": 0.0, "description_ar": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­"}

        try:
            inputs = self.blip_proc(images=frame_pil, text="A video of", return_tensors="pt").to(self.device)
            out = self.blip_model.generate(**inputs, max_length=50)
            description_en = self.blip_proc.decode(out[0], skip_special_tokens=True).strip()
            confidence = 1.0  # BLIP Ù„Ø§ ÙŠØ¹Ø·ÙŠ Ù‚ÙŠÙ…Ø© Ø«Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
            description_ar = translator.translate(description_en) if translator else description_en

            return {"description": description_en, "confidence": confidence, "description_ar": description_ar}
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ BLIP: {e}")
            return {"description": "ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ", "confidence": 0.0, "description_ar": "ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ"}

    def _analyze_videomae(self, frames: List[np.ndarray]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ³Ù„Ø³Ù„ Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¨ÙˆØ§Ø³Ø·Ø© VideoMAE Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø·."""
        if self.videomae_model is None:
            return {"activity": "unknown", "confidence": 0.0, "activity_ar": "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"}

        try:
            # ØªØ£ÙƒØ¯ Ø£Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙŠØ·Ø§Ø¨Ù‚ FRAME_SEQUENCE_LENGTH
            if len(frames) != self.FRAME_SEQUENCE_LENGTH:
                logger.warning(f"âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù„ØªØ³Ù„Ø³Ù„ VideoMAE ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚. Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {self.FRAME_SEQUENCE_LENGTH}, Ø§Ù„ÙØ¹Ù„ÙŠ: {len(frames)}")

            frames_pil = [Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)) for f in frames]
            inputs = self.videomae_proc(frames_pil, return_tensors="pt").to(self.device)

            with torch.no_grad():
                outputs = self.videomae_model(**inputs)

            logits = outputs.logits
            predicted_class_idx = logits.argmax(-1).item()
            confidence = torch.nn.functional.softmax(logits, dim=-1)[0, predicted_class_idx].item()
            activity_label_en = self.videomae_model.config.id2label[predicted_class_idx]
            activity_label_ar = translator.translate(activity_label_en) if translator else activity_label_en

            return {"activity": activity_label_en, "confidence": round(confidence, 4), "activity_ar": activity_label_ar}
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ VideoMAE: {e}")
            return {"activity": "ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„", "confidence": 0.0, "activity_ar": "ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„"}

    def recognize_activity(self, frame: np.ndarray, frame_number: int) -> Dict[str, Any]:
        """
        ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚Øª (buffer) ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø²Ù„Ù‚Ø©.
        Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø®Ø²Ù†ØŒ ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· ÙˆØ§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªÙØ±Ø¬Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
        """
        self.activity_stats["total_frames_processed"] += 1

        if self.videomae_model is None or self.blip_model is None:
            # Ù„Ø§ Ù†Ø²ÙŠØ¯ failed_detections Ù‡Ù†Ø§ Ù„Ø£Ù† Ù‡Ø°Ø§ Ù„ÙŠØ³ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ù†Ø§ÙØ°Ø© ÙƒØ§Ù…Ù„Ø©
            return {
                "activity": "unknown",
                "activity_ar": "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ",
                "description": "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­Ø©",
                "description_ar": "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­Ø©",
                "confidence": 0.0,
                "frame_number": frame_number,
                "status": "error"
            }

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
        self.frame_buffer.append(frame)
        self.frames_since_last_analysis += 1

        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙ‚Ø· Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª ÙƒØ¨ÙŠØ±Ù‹Ø§ Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ (Ø­Ø¬Ù… Ø§Ù„Ù†Ø§ÙØ°Ø©)
        # ÙˆØ¹Ù†Ø¯Ù…Ø§ Ù†ÙƒÙˆÙ† Ù‚Ø¯ ØªÙ‚Ø¯Ù…Ù†Ø§ Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ (Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ‚Ø¯Ù…)
        if len(self.frame_buffer) >= self.FRAME_SEQUENCE_LENGTH and \
           self.frames_since_last_analysis >= self.SLIDING_WINDOW_STRIDE:

            # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· Ø¨ÙˆØ§Ø³Ø·Ø© VideoMAE Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            # Ù†Ø£Ø®Ø° Ø¢Ø®Ø± FRAME_SEQUENCE_LENGTH Ø¥Ø·Ø§Ø±Ù‹Ø§ Ù…Ù† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
            current_window_frames = self.frame_buffer[-self.FRAME_SEQUENCE_LENGTH:]
            videomae_result = self._analyze_videomae(current_window_frames)

            # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø¨ÙˆØ§Ø³Ø·Ø© BLIP (Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø£ÙˆØ³Ø· Ø£Ùˆ Ø§Ù„Ø£Ø®ÙŠØ± Ù…Ù† Ø§Ù„Ù†Ø§ÙØ°Ø©)
            blip_frame = current_window_frames[self.FRAME_SEQUENCE_LENGTH // 2] # Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø§ÙˆØ³Ø· ÙÙŠ Ø§Ù„Ù†Ø§ÙØ°Ø©
            blip_result = self._analyze_blip(Image.fromarray(cv2.cvtColor(blip_frame, cv2.COLOR_BGR2RGB)))

            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            combined_result = {
                "activity": videomae_result.get("activity", "unknown"),
                "activity_ar": videomae_result.get("activity_ar", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"),
                "confidence": videomae_result.get("confidence", 0.0),
                "description": blip_result.get("description", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ")  ,
                "description_ar": blip_result.get("description_ar", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ"),
                "frame_number": frame_number, # Ø±Ù‚Ù… Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ (Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù†Ø§ÙØ°Ø©)
                "status": "success"
            }

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ
            self.scene_history.append(combined_result)
            if len(self.scene_history) > self.max_history:
                self.scene_history.pop(0)

            self.activity_stats["successful_detections"] += 1

            # ØªØ­Ø±ÙŠÙƒ Ø§Ù„Ù†Ø§ÙØ°Ø©: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
            # Ù†Ø­ØªÙØ¸ ÙÙ‚Ø· Ø¨Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙƒØ¨Ø± Ù…Ù† FRAME_SEQUENCE_LENGTHØŒ Ù†Ù‚ÙˆÙ… Ø¨Ù‚ØµÙ‡
            # Ù‡Ø°Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ø§ ÙŠÙ†Ù…Ùˆ Ø¨Ù„Ø§ Ø­Ø¯ÙˆØ¯
            if len(self.frame_buffer) > self.FRAME_SEQUENCE_LENGTH:
                # Ù†Ø­Ø°Ù Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
                self.frame_buffer = self.frame_buffer[len(self.frame_buffer) - self.FRAME_SEQUENCE_LENGTH:]

            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ù†Ø° Ø¢Ø®Ø± ØªØ­Ù„ÙŠÙ„
            self.frames_since_last_analysis = 0

            return combined_result

        else:
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¨Ø¹Ø¯ Ø£Ùˆ Ù„Ù… Ù†ØµÙ„ Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªÙ‚Ø¯Ù…ØŒ Ø§Ø±Ø¬Ø¹ Ù†ØªÙŠØ¬Ø© "pending"
            # ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù‡Ù†Ø§ Ø¥Ø±Ø¬Ø§Ø¹ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª
            return {
                "activity": "pending",
                "activity_ar": "Ø¬Ø§Ø±ÙŠ",
                "confidence": 0.0,
                "description": f"Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ({len(self.frame_buffer)}/{self.FRAME_SEQUENCE_LENGTH})",
                "description_ar": f"Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ({len(self.frame_buffer)}/{self.FRAME_SEQUENCE_LENGTH})",
                "frame_number": frame_number,
                "status": "pending"
            }

    def get_dominant_activity(self) -> Dict[str, Any]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø³Ø§Ø¦Ø¯ ÙˆØ§Ù„ÙˆØµÙ Ø§Ù„Ø³Ø§Ø¦Ø¯ Ù…Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ."""
        if not self.scene_history:
            return {
                "dominant_activity": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ­Ù„ÙŠÙ„",
                "dominant_activity_ar": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ­Ù„ÙŠÙ„",
                "dominant_description": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ",
                "dominant_description_ar": "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ",
                "top_activities": [],
                "top_activities_ar": [],
                "top_descriptions": [],
                "top_descriptions_ar": [],
                "total_samples": 0
            }

        all_activities_en = [item["activity"] for item in self.scene_history if "activity" in item and item["activity"] != "pending"]
        all_activities_ar = [item.get("activity_ar", item["activity"]) for item in self.scene_history if "activity" in item and item["activity"] != "pending"]
        all_descriptions_en = [item["description"] for item in self.scene_history if "description" in item and item["description"] != "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ"]
        all_descriptions_ar = [item.get("description_ar", item["description"]) for item in self.scene_history if "description" in item and item["description"] != "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ"]

        activity_counts_en = Counter(all_activities_en)
        activity_counts_ar = Counter(all_activities_ar)
        description_counts_en = Counter(all_descriptions_en)
        description_counts_ar = Counter(all_descriptions_ar)

        most_common_activities_en = activity_counts_en.most_common(5)
        most_common_activities_ar = activity_counts_ar.most_common(5)
        most_common_descriptions_en = description_counts_en.most_common(5)
        most_common_descriptions_ar = description_counts_ar.most_common(5)

        dominant_activity_en = most_common_activities_en[0][0] if most_common_activities_en else "unknown"
        dominant_activity_ar = most_common_activities_ar[0][0] if most_common_activities_ar else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        dominant_description_en = most_common_descriptions_en[0][0] if most_common_descriptions_en else "no description"
        dominant_description_ar = most_common_descriptions_ar[0][0] if most_common_descriptions_ar else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØµÙ"

        return {
            "dominant_activity": dominant_activity_en,
            "dominant_activity_ar": dominant_activity_ar,
            "dominant_description": dominant_description_en,
            "dominant_description_ar": dominant_description_ar,
            "top_activities": most_common_activities_en,
            "top_activities_ar": most_common_activities_ar,
            "top_descriptions": most_common_descriptions_en,
            "top_descriptions_ar": most_common_descriptions_ar,
            "total_samples": len(self.scene_history)
        }

    def get_activity_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø§Ø·"""
        success_rate = (self.activity_stats["successful_detections"] /
                        self.activity_stats["total_frames_processed"]) * 100 if self.activity_stats[
                                                                                    "total_frames_processed"] > 0 else 0

        return {
            "total_frames_processed": self.activity_stats["total_frames_processed"],
            "successful_detections": self.activity_stats["successful_detections"],
            "failed_detections": self.activity_stats["failed_detections"],
            "success_rate_percentage": round(success_rate, 2),
            "current_history_size": len(self.scene_history),
            "max_history_size": self.max_history
        }

    def get_current_activity_analysis(self, frame: np.ndarray, frame_number: int) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù†Ø´Ø§Ø·"""
        # Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø³ØªØ³ØªØ¯Ø¹ÙŠ recognize_activity Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ¹Ù„ÙŠ
        activity_analysis_per_frame = self.recognize_activity(frame, frame_number)
        stats = self.get_activity_statistics()
        dominant_analysis = self.get_dominant_activity()
        return {
            "activity_analysis_per_frame": activity_analysis_per_frame,
            "dominant_activity_en": dominant_analysis['dominant_activity'],
            "dominant_activity_ar": dominant_analysis['dominant_activity_ar'],
            "dominant_description_en": dominant_analysis['dominant_description'],
            "dominant_description_ar": dominant_analysis['dominant_description_ar'],
            "top_activities_en": dominant_analysis["top_activities"],
            "top_activities_ar": dominant_analysis["top_activities_ar"],
            "top_descriptions_en": dominant_analysis["top_descriptions"],
            "top_descriptions_ar": dominant_analysis["top_descriptions_ar"],
            "statistics": stats,
            "recent_scenes": self.scene_history[-10:] if self.scene_history else []
        }

    def reset_history(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ"""
        self.scene_history.clear()
        self.frame_buffer.clear() # Ù…Ø³Ø­ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ø£ÙŠØ¶Ø§Ù‹
        self.frames_since_last_analysis = 0
        self.activity_stats = { # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            "total_frames_processed": 0,
            "successful_detections": 0,
            "failed_detections": 0
        }
        logger.info("ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø³Ø¬Ù„ ActivityRecognizer")

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.scene_history.clear()
        self.frame_buffer.clear()
        self.frames_since_last_analysis = 0
        self.videomae_proc = None
        self.videomae_model = None
        self.blip_proc = None
        self.blip_model = None
        self.device = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ ActivityRecognizer")

