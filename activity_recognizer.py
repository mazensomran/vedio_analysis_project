import cv2
import numpy as np
import torch
from typing import Dict, Any, List
from collections import Counter
import logging
from model_loader import ModelLoader
from translation_utils import MarianTranslator
from models import VideoEnhancer
from PIL import Image
from qwen_vl_utils import process_vision_info
import traceback

logger = logging.getLogger(__name__)

model_loader = ModelLoader()
translator = MarianTranslator()  # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù‡Ù†Ø§
video_enhancer = VideoEnhancer()

def move_to_device(obj, device):
    """Ù†Ù‚Ù„ ÙƒÙ„ tensors Ø¯Ø§Ø®Ù„ dict/list/tensor Ø¥Ù„Ù‰ Ù†ÙØ³ Ø§Ù„Ø¬Ù‡Ø§Ø²"""
    if torch.is_tensor(obj):
        return obj.to(device)
    elif isinstance(obj, dict):
        return {k: move_to_device(v, device) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [move_to_device(v, device) for v in obj]
    else:
        return obj


class ActivityRecognizer:

    def __init__(self):
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ÙÙ‚Ø·
            self.qwen2_vl_proc, self.qwen2_vl_model = model_loader.load_qwen2_vl_model()
            if self.qwen2_vl_model is None:
                raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL.")

            # device=torch.device("cuda:0")
            self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            # self.device =device  #next(self.qwen2_vl_model.parameters()).device
            self.qwen2_vl_model = self.qwen2_vl_model.to(self.device)
            # self.qwen2_vl_model.parameters().to(device)
            self.qwen2_vl_model.eval()
            print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL Ø¹Ù„Ù‰ {self.device}.")
        except Exception as e:
            self.qwen2_vl_model = None
            self.device = None
            print(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer. ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}   traceback  {traceback.format_exc()}")

    def recognize_activity(self, prompt: str, video_path: str, fsp: float, pixels_size: int,
                           max_new_tokens: int = 600, temperature: float = 0.3,
                           top_p: float = 0.9, top_k: int = 50, do_sample: bool = True,
                           enable_enhancement: bool = False, enhancement_strength: int = 2):

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None
        max_new_tokens = max_new_tokens if max_new_tokens is not None else 600
        temperature = temperature if temperature is not None else 0.3
        top_p = top_p if top_p is not None else 0.9
        top_k = top_k if top_k is not None else 50
        do_sample = do_sample if do_sample is not None else True

        print("âœ… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:")
        print(
            f"max_new_tokens={max_new_tokens}, temperature={temperature}, top_p={top_p}, top_k={top_k}, do_sample={do_sample}")
        print(f"enable_enhancement={enable_enhancement}, enhancement_strength={enhancement_strength}")

        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
        final_video_path = video_path
        if enable_enhancement:
            print("ğŸ¨ ØªÙØ¹ÙŠÙ„ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ...")
            final_video_path = video_enhancer.enhance_video(video_path, enhancement_strength)
            print(f"ğŸ“¹ Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {final_video_path}")

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "video",
                        "video": video_path,  # Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙƒØ§Ù…Ù„
                        "max_pixels": pixels_size * pixels_size,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                        "fps": fsp,
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # ØªÙØ±ÙŠØº Ø°Ø§ÙƒØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        torch.cuda.empty_cache()
        # Preparation for inference
        text = self.qwen2_vl_proc.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.qwen2_vl_proc(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        # inputs = move_to_device(inputs, self.device)
        # device = next(model.parameters()).device
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # inputs = inputs.to(self.device)
        # Inference Ù…Ø¹ no_grad Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        with torch.no_grad():
            generated_ids = self.qwen2_vl_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=do_sample,
            )


        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
        ]
        # ØªÙØ±ÙŠØº ÙÙˆØ±ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        del inputs  # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        torch.cuda.empty_cache()
        # Decode the generated text
        output_text = self.qwen2_vl_proc.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        return output_text[0]

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.qwen2_vl_proc = None
        self.qwen2_vl_model = None
        self.device = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ ActivityRecognizer")
