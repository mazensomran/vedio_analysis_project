
import cv2
import numpy as np
import torch
from typing import Dict, Any, List
from collections import Counter
import logging
from model_loader import ModelLoader
from translation_utils import MarianTranslator
#from qwen2_VL import Qwen2_VL # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
from PIL import Image
from qwen_vl_utils import process_vision_info

logger = logging.getLogger(__name__)

model_loader = ModelLoader()
translator = MarianTranslator() # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù‡Ù†Ø§

class ActivityRecognizer:


    def __init__(self):
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ÙÙ‚Ø·
            self.qwen2_vl_proc, self.qwen2_vl_model = model_loader.load_qwen2_vl_model()

            if self.qwen2_vl_model is None:
                raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL.")

            self.device = next(self.qwen2_vl_model.parameters()).device
            self.qwen2_vl_model.eval()
            print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL.")
        except Exception as e:
            self.qwen2_vl_model = None
            self.device = None
            print(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© ActivityRecognizer. ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")


    def recognize_activity(self,prompt:str, video_path: str, fsp: float, pixels_size: int):
        """
        ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· ÙˆØ§Ù„Ø¨ÙŠØ¦Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Qwen2-VL.
        ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯ Ø£Ùˆ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª.
        """
        messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "video",
                            "video": video_path,  # Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙƒØ§Ù…Ù„
                                "max_pixels": pixels_size * pixels_size,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                        "fps": fsp,
                        },
                        {"type": "text", "text": prompt},
                    ],
                }
            ]

        # ØªÙØ±ÙŠØº Ø°Ø§ÙƒØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        torch.cuda.empty_cache()
        # Preparation for inference
        text = self.qwen2_vl_proc.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.qwen2_vl_proc( 
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device) 
        # Inference Ù…Ø¹ no_grad Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        with torch.no_grad():
            generated_ids = self.qwen2_vl_model.generate( 
                **inputs,
                max_new_tokens=150
            )
        # Trim the generated output to remove the input prompt
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        # ØªÙØ±ÙŠØº ÙÙˆØ±ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„
        del inputs  # Ø­Ø°Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        torch.cuda.empty_cache()
        # Decode the generated text
        output_text = self.qwen2_vl_proc.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        return output_text[0]
    
    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.qwen2_vl_proc = None
        self.qwen2_vl_model = None
        self.device = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ ActivityRecognizer")
