import cv2
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import subprocess
import numpy as np
from datetime import datetime
import torch
import time
import threading
from translation_utils import MarianTranslator
from config import OUTPUTS_DIR, PROCESSING_CONFIG, MODEL_CONFIG
from database import db
from models import FaceDetector, TextDetector, SpeechRecognizer, GeneralObjectDetector, ObjectTracker, FrameEnhancer
from activity_recognizer import ActivityRecognizer
from model_loader import model_loader
from monitoring import ProcessMonitor
import logging

logger = logging.getLogger(__name__)
monitor = ProcessMonitor()

stop_processing = False
current_process_id = None
translator = MarianTranslator()

TARGET_FACE_WIDTH = PROCESSING_CONFIG["target_face_width"]
TARGET_FACE_HEIGHT = PROCESSING_CONFIG["target_face_height"]
FACE_PADDING_RATIO = PROCESSING_CONFIG["face_padding_ratio"]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

enhancer = FrameEnhancer(model_path='RealESRGAN_x4plus.pth')

def set_stop_processing(value: bool, process_id: Optional[str] = None):
    """ØªØ¹ÙŠÙŠÙ† Ø­Ø§Ù„Ø© Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    global stop_processing, current_process_id
    stop_processing = value
    if process_id:
        current_process_id = process_id


def get_stop_processing():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    return stop_processing


def setup_output_dirs(process_id: str) -> Tuple[Path, Path, Path]:
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    process_dir = OUTPUTS_DIR / process_id
    faces_dir = process_dir / "faces"
    video_dir = process_dir / "video"
    audio_dir = process_dir / "audio"

    for directory in [process_dir, faces_dir, video_dir, audio_dir]:
        directory.mkdir(exist_ok=True, parents=True)

    return process_dir, faces_dir, video_dir

def extract_audio(video_path: str, output_audio_path: str) -> bool:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
    try:
        command = [
            'ffmpeg', '-i', video_path, '-q:a', '0', '-map', 'a',
            output_audio_path, '-y', '-loglevel', 'error'
        ]
        result = subprocess.run(command, capture_output=True, text=True)

        if result.returncode == 0:
            logger.info(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª Ø¨Ù†Ø¬Ø§Ø­: {output_audio_path}")
            return True
        else:
            logger.info(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {result.stderr}")
            return False

    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª: {e}")
        return False


def find_track_id_for_bbox(bbox, tracks, iou_threshold=0.3):
    """
    ØªØ¨Ø­Ø« Ø¹Ù† track_id ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© tracks Ø§Ù„Ø°ÙŠ ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ bbox Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³ IoU.

    Returns:
        track_id Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¥Ø°Ø§ ÙˆØ¬Ø¯ØŒ Ø£Ùˆ None Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚.
    """

    def iou(boxA, boxB):
        xA = max(boxA[0], boxB[0])
        yA = max(boxA[1], boxB[1])
        xB = min(boxA[2], boxB[2])
        yB = min(boxA[3], boxB[3])
        interArea = max(0, xB - xA) * max(0, yB - yA)
        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
        unionArea = boxAArea + boxBArea - interArea
        if unionArea == 0:
            return 0
        return interArea / unionArea

    best_iou = 0
    best_track_id = None
    for track in tracks:
        track_bbox = track['bbox']
        current_iou = iou(bbox, track_bbox)
        if current_iou > best_iou and current_iou >= iou_threshold:
            best_iou = current_iou
            best_track_id = track['track_id']
    return best_track_id

def process_single_frame(frame: np.ndarray, frame_number: int,
                        face_detector: FaceDetector, text_detector: TextDetector,
                        object_detector: GeneralObjectDetector, object_tracker: ObjectTracker,
                        activity_recognizer: ActivityRecognizer,
                        options: Dict[str, Any], process_id: str, faces_dir: Path) -> Tuple[np.ndarray, List, List, List, List, Dict, List]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯ Ù…Ø¹ ÙƒØ´Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆØªØªØ¨Ø¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ"""

    processed_frame = frame.copy()
    all_objects = []
    all_faces = []
    all_texts = []
    all_tracks = []
    activity_data = {}
    persons_data = []

    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø·Ø§Ø±
    enhanced_img_pil = enhancer.enhance_frame(frame)
    enhanced_frame = cv2.cvtColor(np.array(enhanced_img_pil), cv2.COLOR_RGB2BGR)

    # ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡
    if options.get("enable_face_detection", True) and face_detector:
        faces = face_detector.detect_faces(enhanced_frame, frame_number)
        all_faces.extend(faces)
        for j, face in enumerate(faces):
            bbox = face["bbox"]
            x, y, w, h = bbox

            # Ø§Ø¶Ø§ÙØ© Ø­Ø´Ùˆ Ù„Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ù…ÙƒØªØ´ÙØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø§ÙØ¶Ù„
            pad_x = int(w * FACE_PADDING_RATIO / 2)
            pad_y = int(h * FACE_PADDING_RATIO / 2)

            x1_padded = max(0, x - pad_x)
            y1_padded = max(0, y - pad_y)
            x2_padded = min(frame.shape[1], x + w + pad_x)
            y2_padded = min(frame.shape[0], y + h + pad_y)

            face_img = frame[y1_padded:y2_padded, x1_padded:x2_padded]

            if face_img.shape[0] > 0 and face_img.shape[1] > 0:
                current_height, current_width = face_img.shape[:2]
                if current_width < TARGET_FACE_WIDTH or current_height < TARGET_FACE_HEIGHT:
                    face_img = cv2.resize(face_img, (
                    max(current_width * 2, TARGET_FACE_WIDTH), max(current_height * 2, TARGET_FACE_HEIGHT)),
                                          interpolation=cv2.INTER_CUBIC)
                face_img_resized = cv2.resize(face_img, (TARGET_FACE_WIDTH, TARGET_FACE_HEIGHT),
                                              interpolation=cv2.INTER_AREA) #ØªÙˆØ­ÙŠØ¯ Ù‚ÙŠØ§Ø³ ØµÙˆØ± Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©

                face_filename = f"face_{frame_number}_{j}.jpg"
                face_path = faces_dir / face_filename
                try:
                    cv2.imwrite(str(face_path), face_img_resized)
                    face["image_path"] = str(face_path.relative_to(OUTPUTS_DIR / process_id / "faces"))
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ ØµÙˆØ±Ø© Ø§Ù„ÙˆØ¬Ù‡: {e}")
                    face["image_path"] = ""
            else:
                logger.info(f"âš ï¸ ØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙˆØ¬Ù‡ ÙØ§Ø±Øº ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}ØŒ Ù„Ù† ÙŠØªÙ… Ø­ÙØ¸Ù‡.")
                face["image_path"] = ""

    # ÙƒØ´Ù Ø§Ù„Ù†ØµÙˆØµ
    if options.get("enable_text_detection", True) and text_detector:
        texts = text_detector.detect_text(enhanced_frame, frame_number)
        all_texts.extend(texts)

    # ÙƒØ´Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª
    all_detections = []
    if object_detector:
        all_detections = object_detector.detect_objects(enhanced_frame, frame_number)

    # ÙØµÙ„ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙˆØªØªØ¨Ø¹Ù‡Ù…
    person_detections = [d for d in all_detections if d["class_name"] == "person"]

    tracks = []
    if options.get("enable_tracking", True) and object_tracker:
        tracks = object_tracker.track_objects(frame, person_detections)
        all_tracks.extend(tracks)

    # Ø±Ø³Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª
    for det in all_detections:
        bbox = det["bbox"]
        class_name = det["class_name"]
        confidence = det["confidence"]
        color = (0, 255, 0) if class_name != "person" else (255, 0, 0)
        x1, y1, x2, y2 = map(int, bbox)
        cv2.rectangle(processed_frame, (x1, y1), (x2, y2), color, 2)
        label = f"{class_name} {confidence:.2f}"
        cv2.putText(processed_frame, label, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # Ø±Ø³Ù… Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø´Ø®Ø§Øµ
    if all_tracks:
        processed_frame = object_tracker.draw_tracks(processed_frame, all_tracks)

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· (ActivityRecognizer ÙŠØ¯ÙŠØ± Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø²Ù„Ù‚Ø© Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§)
    if options.get("enable_activity_recognition", True) and activity_recognizer:
        activity_data = activity_recognizer.recognize_activity(enhanced_frame, frame_number)

    # Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ù…Ø¹ Ø±Ø¨Ø· track_id Ù„Ù„Ø£Ø´Ø®Ø§Øµ
    for det in all_detections:
        track_id = None
        if det["class_name"] == "person":
            track_id = find_track_id_for_bbox(det["bbox"], all_tracks)
            bbox_to_save = det["bbox"].tolist()
            x1, y1, x2, y2 = bbox_to_save
            persons_data.append({
                "process_id": process_id,
                "frame_number": frame_number,
                "class_name": det["class_name"],
                "bbox_x1": float(x1),
                "bbox_y1": float(y1),
                "bbox_x2": float(x2),
                "bbox_y2": float(y2),
                "confidence": float(det["confidence"]),
                "track_id": track_id
            })
        else: # Other objects
            bbox_to_save = det["bbox"].tolist()
            x1, y1, x2, y2 = bbox_to_save
            all_objects.append({
                "process_id": process_id,
                "frame_number": frame_number,
                "class_name": det["class_name"],
                "bbox_x1": float(x1),
                "bbox_y1": float(y1),
                "bbox_x2": float(x2),
                "bbox_y2": float(y2),
                "confidence": float(det["confidence"]),
                "track_id": None # Ù„Ø§ ÙŠÙˆØ¬Ø¯ track_id Ù„Ù„ÙƒØ§Ø¦Ù†Ø§Øª ØºÙŠØ± Ø§Ù„Ø£Ø´Ø®Ø§Øµ
            })

    return processed_frame, all_objects, all_faces, all_texts, all_tracks, activity_data, persons_data


def monitor_processing(process_id: str, total_frames: int, cap: cv2.VideoCapture):
    """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    error_count = 0
    max_errors = PROCESSING_CONFIG.get("max_404_retries", 20)

    while not get_stop_processing():
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            process_info = db.get_process_status(process_id)
            if not process_info or process_info["status"] in ["completed", "error", "stopped"]:
                break

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…ÙØªÙˆØ­Ø§Ù‹
            if not cap.isOpened():
                error_count += 1
                print(f"âš ï¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…ØºÙ„Ù‚ - Ø§Ù„Ø®Ø·Ø£ Ø±Ù‚Ù… {error_count}")

            # Ø¥Ø°Ø§ ØªØ¬Ø§ÙˆØ²Øª Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
            if error_count >= max_errors:
                print(f"âŒ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø³Ø¨Ø¨ {max_errors} Ø£Ø®Ø·Ø§Ø¡ Ù…ØªØªØ§Ù„ÙŠØ©")
                set_stop_processing(True, process_id)
                db.update_process_status(process_id, "error", 0,
                                         f"ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø³Ø¨Ø¨ {max_errors} Ø£Ø®Ø·Ø§Ø¡ Ù…ØªØªØ§Ù„ÙŠØ©")
                break

            time.sleep(2)  # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù…Ø¯Ø© Ø«Ø§Ù†ÙŠØªÙŠÙ† Ø¨ÙŠÙ† ÙƒÙ„ ÙØ­Øµ

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©: {e}")
            error_count += 1
            time.sleep(2)

def convert_numpy_types(obj):
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(i) for i in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64)):
        return int(obj)
    else:
        return obj


def process_video(input_path: str, process_id: str, options: Dict[str, Any]):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª GPU ÙˆØ§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙˆØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ÙˆØ¬ÙˆÙ‡"""
    start_time = time.time()

    global stop_processing

    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
    text_detector = None
    face_detector = None
    speech_recognizer = None
    activity_recognizer = None
    object_tracker = None
    object_detector = None
    # face_enhancer = None # Ù„Ù… ÙŠØ¹Ø¯ ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ù†Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø±

    cap = None
    out = None

    try:
        set_stop_processing(False, process_id)

        process_dir, faces_dir, video_dir = setup_output_dirs(process_id)

        db.update_process_status(process_id, "processing", 5, "Ø¬Ø§Ø±ÙŠ ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")

        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            db.update_process_status(process_id, "error", 0, "ØªØ¹Ø°Ø± ÙØªØ­ Ù…Ù„Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
            return

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / fps if fps > 0 else 0

        print(f"ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {total_frames} Ø¥Ø·Ø§Ø±, {fps:.2f} FPS, {width}x{height}")
        print(f"â±ï¸ Ø§Ù„Ù…Ø¯Ø©: {duration:.2f} Ø«Ø§Ù†ÙŠØ©")

        monitor_thread = threading.Thread(
            target=monitor_processing,
            args=(process_id, total_frames, cap),
            daemon=True
        )
        monitor_thread.start()

        output_video_path = str(video_dir / "analyzed_video.mp4")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

        db.update_process_status(process_id, "processing", 10, "Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            import gc
            gc.collect()

        face_detector = FaceDetector() if options.get("enable_face_detection", True) else None
        text_detector = TextDetector() if options.get("enable_text_detection", True) else None
        object_tracker = ObjectTracker() if options.get("enable_tracking", True) else None
        activity_recognizer = ActivityRecognizer() if options.get("enable_activity_recognition", True) else None
        speech_recognizer = SpeechRecognizer() if options.get("enable_audio_transcription", True) else None

        object_detector = GeneralObjectDetector(model_name="yolov8s.pt", device=MODEL_CONFIG["device"])

        db.update_process_status(process_id, "processing", 15, "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")

        transcription_result = None
        if options.get("enable_audio_transcription", True) and speech_recognizer:
            db.update_process_status(process_id, "processing", 20, "Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª")
            audio_path = str(process_dir / "audio" / "extracted_audio.mp3")

            if extract_audio(input_path, audio_path):
                db.update_process_status(process_id, "processing", 25, "Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ")
                transcription_result = speech_recognizer.transcribe_audio(audio_path)

                if transcription_result and transcription_result["text"]:
                    db.add_transcription(process_id, transcription_result)
                    print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {len(transcription_result['text'])} Ø­Ø±Ù")
                else:
                    print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Øµ ÙÙŠ Ø§Ù„ØµÙˆØª")
            else:
                print("âš ï¸ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª")

            speech_recognizer.cleanup()
            model_loader.clear_model_cache(f"whisper_{MODEL_CONFIG['speech_recognition_model']}")
            del speech_recognizer
            speech_recognizer = None

        frame_number = 0
        all_faces = []
        all_texts = []
        all_tracking_data = []
        all_activities = [] # Ù„ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· Ù…Ù† recognize_activity
        all_objects = []
        all_people = []

        db.update_process_status(process_id, "processing", 30, "Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø·Ø§Ø±Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")

        while True:
            if get_stop_processing():
                print("â¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙŠØ¯ÙˆÙŠØ§Ù‹")
                db.update_process_status(process_id, "stopped",
                                         int((frame_number / total_frames) * 65) + 30,
                                         "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
                break

            monitor.start_monitoring()
            ret, frame = cap.read()
            if not ret:
                break

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©
            processed_frame, current_objects, current_faces, current_texts, current_tracks, current_activity_data, current_persons = \
                process_single_frame(frame, frame_number, face_detector, text_detector,
                                     object_detector, object_tracker, activity_recognizer,
                                     options, process_id, faces_dir)

            out.write(processed_frame)

            all_objects.extend(current_objects)
            all_faces.extend(current_faces)
            all_texts.extend(current_texts)
            all_tracking_data.extend(current_tracks)
            all_people.extend(current_persons)

            # Ø¥Ø¶Ø§ÙØ© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù„ÙŠØ³Øª "pending"
            if current_activity_data and current_activity_data.get("status") == "success":
                all_activities.append(current_activity_data)

            frame_number += 1

            if frame_number % 10 == 0:
                progress = 30 + int((frame_number / total_frames) * 65)
                progress = min(progress, 95)
                db.update_process_status(process_id, "processing", progress,
                                         f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}/{total_frames}")

        # Ø¨Ø¹Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§ØªØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø¥Ø·Ø§Ø±Ø§Øª Ù…ØªØ¨Ù‚ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ ActivityRecognizer
        # ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ù„Ø§ ØªØ´ÙƒÙ„ Ù†Ø§ÙØ°Ø© ÙƒØ§Ù…Ù„Ø©. Ù„Ø§ Ù†Ø­ØªØ§Ø¬ Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù‡Ù†Ø§ Ù„Ø£Ù† ActivityRecognizer ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§.

        if get_stop_processing():
            print("ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù")
            final_progress = 30 + int((frame_number / total_frames) * 65)
            db.update_process_status(process_id, "stopped", final_progress,
                                     "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ©")
        else:
            db.update_process_status(process_id, "processing", 95, "Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©")

        # Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ù…Ø­Ø³Ù†Ø© (Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ù„Ù… ÙŠØªØºÙŠØ±)
        enhanced_faces = [face for face in all_faces if face.get("enhanced", False)]
        if enhanced_faces:
            print(f"âœ¨ ØªÙ… ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© {len(enhanced_faces)} ÙˆØ¬Ù‡ Ù…Ù† Ø£ØµÙ„ {len(all_faces)}")

        if all_faces:
            faces_output_path = process_dir / "faces_data.json"
            with open(faces_output_path, 'w', encoding='utf-8') as f:
                json.dump(all_faces, f, ensure_ascii=False, indent=2)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_faces)} ÙˆØ¬Ù‡ ÙÙŠ Ù…Ù„Ù JSON")

        if all_texts:
            texts_output_path = process_dir / "texts_data.json"
            with open(texts_output_path, 'w', encoding='utf-8') as f:
                json.dump(all_texts, f, ensure_ascii=False, indent=2)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_texts)} Ù†Øµ ÙÙŠ Ù…Ù„Ù JSON")

        if all_tracking_data:
            tracking_output_path = process_dir / "tracking_data.json"
            with open(tracking_output_path, 'w', encoding='utf-8') as f:
                json.dump(convert_numpy_types(all_tracking_data), f, ensure_ascii=False, indent=2)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_tracking_data)} Ø¹Ù…Ù„ÙŠØ© ØªØªØ¨Ø¹ ÙÙŠ Ù…Ù„Ù JSON")

        if all_objects:
            objects_output_path = process_dir / "objects_data.json"
            with open(objects_output_path, 'w', encoding='utf-8') as f:
                json.dump(convert_numpy_types(all_objects), f, ensure_ascii=False, indent=2)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(all_objects)} ÙƒØ§Ø¦Ù† ÙÙŠ Ù…Ù„Ù JSON")

        objects = list(set(obj["class_name"] for obj in all_objects))
        objects_ar = [translator.translate(obj) for obj in objects]
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†Ø´Ø§Ø· Ù…Ù† ActivityRecognizer
        # Ù„Ø§ Ù†Ø­ØªØ§Ø¬ Ù„ØªÙ…Ø±ÙŠØ± Ø¥Ø·Ø§Ø± ÙˆÙ‡Ù…ÙŠ Ù‡Ù†Ø§ØŒ ÙÙ‚Ø· Ù†Ø·Ù„Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ø§ÙƒÙ…ÙŠ
        final_activity_analysis = activity_recognizer.get_dominant_activity()
        activity_stats = activity_recognizer.get_activity_statistics()

        activity_output = {
            "dominant_activity_en": final_activity_analysis.get("dominant_activity"),
            "dominant_activity_ar": final_activity_analysis.get("dominant_activity_ar"),
            "dominant_description_en": final_activity_analysis.get("dominant_description"),
            "dominant_description_ar": final_activity_analysis.get("dominant_description_ar"),
            "top_activities_en": final_activity_analysis.get("top_activities"),
            "top_activities_ar": final_activity_analysis.get("top_activities_ar"),
            "top_descriptions_en": final_activity_analysis.get("top_descriptions"),
            "top_descriptions_ar": final_activity_analysis.get("top_descriptions_ar"),
            "statistics": activity_stats, # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
            "recent_scenes": activity_recognizer.scene_history[-10:] if activity_recognizer.scene_history else [],
            "per_frame_results": all_activities # Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡Ø§ Ù…Ù† ÙƒÙ„ Ù†Ø§ÙØ°Ø© Ù…ÙƒØªÙ…Ù„Ø©
        }

        unique_activities = list({item["activity"] for item in all_activities if "activity" in item})
        unique_descriptions = list({item["description"] for item in all_activities if "description" in item})

        # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙØ±ÙŠØ¯Ø©
        unique_activities_ar = [translator.translate(act) for act in unique_activities]
        # ØªØ±Ø¬Ù…Ø© Ø§Ù„ØªÙˆØµÙŠÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©
        unique_descriptions_ar = [translator.translate(desc) for desc in unique_descriptions]

        activity_file = process_dir / "activity_analysis.json"
        with open(activity_file, "w", encoding="utf-8") as f:
            json.dump(activity_output, f, ensure_ascii=False, indent=2)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ {activity_file}")

        # Ø­ÙØ¸ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø³Ø§Ø¦Ø¯ ÙˆØ§Ù„ÙˆØµÙ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if activity_output.get("dominant_activity_en") and activity_output.get("dominant_description_en"):
            db.add_scene_analysis(
                process_id=process_id,
                scene_info={
                    "activity": activity_output["dominant_activity_en"],
                    "activity_ar": activity_output["dominant_activity_ar"],
                    "description": activity_output["dominant_description_en"],
                    "description_ar": activity_output["dominant_description_ar"],
                    "confidence": 1.0, # Ø§Ù„Ø«Ù‚Ø© Ù‡Ù†Ø§ Ù‡ÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¦Ø¯ØŒ ÙˆÙ„ÙŠØ³Øª Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙØ±Ø¯ÙŠØ©
                    "frame_number": frame_number # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø£Ø®ÙŠØ± Ø§Ù„Ø°ÙŠ ØªÙ… Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡
                }
            )

        end_time = time.time()
        processing_duration = end_time - start_time

        results = {
            "process_id": process_id,
            "total_frames": total_frames,
            "frames_processed": frame_number,
            "fps": fps,
            "resolution": f"{width}x{height}",
            "duration_seconds": duration,
            "faces_detected": len(all_faces),
            "faces_enhanced": len(enhanced_faces),
            "poeple_detected": len(all_people),
            "texts_detected": len(all_texts),
            "tracks_detected": len(set(track["track_id"] for track in all_tracking_data if "track_id" in track and track["track_id"] is not None)),
            "objects_detected": (len(objects), objects),
            "objects_ar":objects_ar,
            "transcription": transcription_result,
            "activity_analysis": {
            "dominant_activity_en": activity_output.get("dominant_activity_en") if activity_recognizer else None,
            "dominant_activity_ar": activity_output.get("dominant_activity_ar") if activity_recognizer else None,
            "dominant_description_en": activity_output.get("dominant_description_en") if activity_recognizer else None,
            "dominant_description_ar": activity_output.get("dominant_description_ar") if activity_recognizer else None,
            "top_activities": activity_output.get("top_activities_en") if activity_recognizer else [],
            "top_descriptions": activity_output.get("top_descriptions_en") if activity_recognizer else [],
            "top_activities_ar": activity_output.get("top_activities_ar") if activity_recognizer else [],
            "top_descriptions_ar": activity_output.get("top_descriptions_ar") if activity_recognizer else [],
            "statistics": activity_output.get("statistics") if activity_recognizer else {},
            "unique_activities": unique_activities,
            "unique_activities_ar": unique_activities_ar,
            "unique_descriptions": unique_descriptions,
            "unique_descriptions_ar": unique_descriptions_ar,
        },
            "processing_date": datetime.now().isoformat(),
            "processing_options": options,
            "face_enhancement_enabled": options.get("enable_face_enhancement", False),
            "processing_status": "stopped" if get_stop_processing() else "completed",
            "processing_duration_seconds": processing_duration

        }

        results_file = process_dir / "final_results.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ {results_file}")

        if cap:
            cap.release()
        if out:
            out.release()

        model_loader.cleanup()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        if not get_stop_processing():
            db.update_process_status(process_id, "completed", 100, "ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­")
            monitor.remove_process(process_id)
            print("ğŸ‰ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­!")

    except Exception as e:
        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"
        monitor.report_error(process_id)
        print(f"âŒ {error_msg}")

        try:
            current_progress = 30 + int((frame_number / total_frames) * 65) if 'frame_number' in locals() else 0
            db.update_process_status(process_id, "error", current_progress, error_msg)
        except:
            print("âš ï¸ Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        model_loader.cleanup()

    finally:
        try:
            if cap and cap.isOpened():
                cap.release()
            if out:
                out.release()
            try:
                model_loader.cleanup()
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯: {e}")

            if text_detector and hasattr(text_detector, 'cleanup'):
                text_detector.cleanup()
            if face_detector and hasattr(face_detector, 'cleanup'):
                face_detector.cleanup()
            if object_tracker and hasattr(object_tracker, 'cleanup'):
                object_tracker.cleanup()
            if activity_recognizer and hasattr(activity_recognizer, 'cleanup'):
                activity_recognizer.cleanup()
            if speech_recognizer and hasattr(speech_recognizer, 'cleanup'):
                speech_recognizer.cleanup()

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {e}")

        set_stop_processing(False, None)


def get_processing_status(process_id: str) -> Tuple[Dict[str, Any], str, str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    try:
        process_info = db.get_process_status(process_id)

        if not process_info:
            return {}, "not_found", "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©"

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù…Ù„Ù final_results.json
        process_dir = OUTPUTS_DIR / process_id
        results_file = process_dir / "final_results.json"

        results = {}
        if results_file.exists():
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø© ÙˆØ§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        results["status"] = process_info["status"]
        results["message"] = process_info["message"]
        results["progress"] = process_info["progress"]

        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù…ÙƒØªÙ…Ù„Ø© Ø£Ùˆ Ù…ØªÙˆÙ‚ÙØ©
        if process_info["status"] in ["completed", "stopped"]:
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
            faces_file = process_dir / "faces_data.json"
            if faces_file.exists():
                with open(faces_file, 'r', encoding='utf-8') as f:
                    results["faces_data"] = json.load(f)
            else:
                results["faces_data"] = []

            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙˆØµ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
            texts_file = process_dir / "texts_data.json"
            if texts_file.exists():
                with open(texts_file, 'r', encoding='utf-8') as f:
                    results["extracted_texts"] = json.load(f)
            else:
                results["extracted_texts"] = []

            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØªØ¨Ø¹ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
            tracking_file = process_dir / "tracking_data.json"
            if tracking_file.exists():
                with open(tracking_file, 'r', encoding='utf-8') as f:
                    results["tracking_data"] = json.load(f)
            else:
                results["tracking_data"] = []

            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
            activity_file = process_dir / "activity_analysis.json"
            if activity_file.exists():
                with open(activity_file, 'r', encoding='utf-8') as f:
                    activity_data = json.load(f)
                    results["activity_analysis"] = activity_data # ØªÙ… ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ù„ÙŠØ¹ÙƒØ³ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            else:
                results["activity_analysis"] = {}

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª
            if "duration_seconds" not in results:
                results["duration_seconds"] = 0
            if "total_frames" not in results:
                results["total_frames"] = 0
            if "fps" not in results:
                results["fps"] = 0

        return results, process_info["status"], process_info["message"]

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
        return {}, "error", f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø©: {str(e)}"


def stop_video_processing(process_id: str):
    """Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
    global stop_processing, current_process_id

    if current_process_id == process_id:
        set_stop_processing(True, process_id)
        return True
    else:
        print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù…Ù„ÙŠØ© Ù†Ø´Ø·Ø© Ø¨Ø§Ù„Ù…Ø¹Ø±Ù {process_id}")
        return False


def cleanup_processing():
    """ØªÙ†Ø¸ÙŠÙ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    global stop_processing, current_process_id
    set_stop_processing(True, None)
    current_process_id = None

    # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© GPU
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

