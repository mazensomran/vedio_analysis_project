
import torch
from transformers import AutoImageProcessor
import whisper
import os
from pathlib import Path
import requests
import time
from typing import Optional, List, Dict, Any, Tuple
import logging
from config import MODELS_DIR, MODEL_CONFIG, PROCESSING_CONFIG, GPU_AVAILABLE
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
from scrfd import SCRFD, Threshold
import torch
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger = logging.getLogger(__name__)


class ModelLoader:
    def __init__(self):
        self.device = torch.device(MODEL_CONFIG["device"])
        self.models_dir = MODELS_DIR
        self.models_dir.mkdir(exist_ok=True)
        self.loaded_models = {}
        self.model_cache = {}

        print(f"ğŸ¯ Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.device}")
        if GPU_AVAILABLE:
            print(f"ğŸ¯ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

    def _download_model(self, model_url: str, model_path: Path) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† URL"""
        try:
            print(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {model_url}")

            response = requests.get(model_url, stream=True, timeout=MODEL_CONFIG["model_download_timeout"])
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0

            with open(model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {progress:.1f}%", end='\r')

            print(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­: {model_path.name}")
            return True

        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            if model_path.exists():
                model_path.unlink()
            return False

    def _setup_model_cache(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if MODEL_CONFIG["optimize_for_inference"]:
            torch.backends.cudnn.benchmark = True
            if GPU_AVAILABLE:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True


    def load_scrfd_model(self, model_path: Optional[Path] = None) -> Optional[SCRFD]:
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙÙ…Ø±Ø± Ù…Ø³Ø§Ø±ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if model_path is None:
            model_path = MODEL_CONFIG["scrfd_model_path"]
        # ØªØ­ÙˆÙŠÙ„ model_path Ø¥Ù„Ù‰ ÙƒØ§Ø¦Ù† Path Ø¥Ø°Ø§ ÙƒØ§Ù† str
        if isinstance(model_path, str):
            model_path = Path(model_path)
        # (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cache_key = f"scrfd_{model_path.name}"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]
        try:
            if not model_path.exists():
                print(f"âš ï¸ Ù†Ù…ÙˆØ°Ø¬ SCRFD ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {model_path}. ÙŠØ±Ø¬Ù‰ ØªÙ†Ø²ÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠØ§Ù‹.")
                return None
            print(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SCRFD Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠ: {model_path.name}")
            model = SCRFD.from_path(str(model_path))
            self.model_cache[cache_key] = model
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SCRFD Ø¨Ù†Ø¬Ø§Ø­.")
            return model
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ SCRFD: {e}")
            return None

    def load_whisper_model(self, model_name: str = None) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper"""
        if model_name is None:
            model_name = MODEL_CONFIG["speech_recognition_model"]

        cache_key = f"whisper_{model_name}"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]

        try:
            print(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper: {model_name}")

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­
            available_models = MODEL_CONFIG["available_whisper_models"]
            if model_name not in available_models:
                print(f"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… 'base' Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†Ù‡")
                model_name = "base"

            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… float32
            model = whisper.load_model(
                model_name,
                download_root=str(self.models_dir / "whisper"),
                device=self.device
            )

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… float32 Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† half Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            model = model.float()

            self.model_cache[cache_key] = model
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Whisper ({model_name}) Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {self.device}")
            return model

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Whisper: {e}")
            return None

    def load_qwen2_vl_model(self, model_name: str = "Qwen/Qwen2-VL-2B-Instruct"):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ"""
        cache_key = f"qwen2_vl_{model_name}"
        if cache_key in self.model_cache:
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ({model_name}) ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
            return self.model_cache[cache_key]
        try:
            logger.info(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {model_name}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… AutoProcessor Ùˆ AutoModelForVision2Seq
            self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct", torch_dtype="auto")
            model.to( self.device )
            # default processer
            processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
            self.model_cache[cache_key] = (processor, model)
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {model_name} Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {self.device}")
            return processor, model
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL: {e}")
            return None, None


    def clear_model_cache(self, model_key: str = None):
        """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if model_key:
            if model_key in self.model_cache:
                # ØªÙØ±ÙŠØº Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                model_instance = self.model_cache[model_key]
                if isinstance(model_instance, tuple): # Ø¥Ø°Ø§ ÙƒØ§Ù† tuple (Ù…Ø«Ù„ BLIP: processor, model)
                    for item in model_instance:
                        del item
                else:
                    del model_instance
                del self.model_cache[model_key]
                logger.info(f"ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {model_key}")
        else:
            # Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            for key in list(self.model_cache.keys()):
                self.clear_model_cache(key) # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ù†ÙØ³Ù‡Ø§ Ù„ØªÙØ±ÙŠØº ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø­Ø¯Ø©
            if GPU_AVAILABLE:
                torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©")

    def get_model_memory_usage(self) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        memory_usage = {}

        for model_name, model in self.model_cache.items():
            try:
                if hasattr(model, 'parameters'):
                    # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                    total_size = (param_size + buffer_size) / 1024 ** 2  # MB

                    memory_usage[model_name] = total_size
            except:
                pass

        return memory_usage

    def load_scene_recognition_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Qwen2-VL ÙÙ‚Ø· Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ù‡Ø¯/Ø§Ù„Ù†Ø´Ø§Ø·."""
        qwen2_vl_processor, qwen2_vl_model = self.load_qwen2_vl_model()
        # Ù†ÙØ±Ø¬Ø¹ Ù‚ÙŠÙ…ØªÙŠÙ† ÙÙ‚Ø· Ø§Ù„Ø¢Ù†
        return qwen2_vl_processor, qwen2_vl_model

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.model_cache.clear()
        if GPU_AVAILABLE:
            torch.cuda.empty_cache()
        print("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø±Ø¯ ModelLoader")


def load_text_recognition_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
    # Ø³ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ EasyOCR ÙÙŠ Ù…ÙƒØ§Ù† Ø¢Ø®Ø±
    return None


def load_speech_recognition_model():
    """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…"""
    return model_loader.load_whisper_model()


# Ø¥Ù†Ø´Ø§Ø¡ loader Ø¹Ø§Ù„Ù…ÙŠ
model_loader = ModelLoader()

def cleanup_models():
    """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    model_loader.cleanup()

