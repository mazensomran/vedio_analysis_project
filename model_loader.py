import torch
from ultralytics import YOLO
import whisper
from pathlib import Path
import requests
from typing import Optional, List, Dict, Any, Tuple
import logging
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import AutoFeatureExtractor, VideoMAEForVideoClassification
from config import MODELS_DIR, MODEL_CONFIG, PROCESSING_CONFIG, GPU_AVAILABLE

logger = logging.getLogger(__name__)


class ModelLoader:
    def __init__(self):
        self.device = torch.device(MODEL_CONFIG["device"])
        self.models_dir = MODELS_DIR
        self.models_dir.mkdir(exist_ok=True)
        self.loaded_models = {}
        self.model_cache = {}

        logger.info(f"ğŸ¯ Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {self.device}")
        if GPU_AVAILABLE:
            print(f"ğŸ¯ Ø°Ø§ÙƒØ±Ø© GPU Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

    def _download_model(self, model_url: str, model_path: Path) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† URL"""
        try:
            logger.info(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {model_url}")

            response = requests.get(model_url, stream=True, timeout=MODEL_CONFIG["model_download_timeout"])
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0

            with open(model_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            logger.info(f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {progress:.1f}%", end='\r')

            logger.info(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­: {model_path.name}")
            return True

        except Exception as e:
            logger.info(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            if model_path.exists():
                model_path.unlink()
            return False

    def _setup_model_cache(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if MODEL_CONFIG["optimize_for_inference"]:
            torch.backends.cudnn.benchmark = True
            if GPU_AVAILABLE:
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True

    def load_yolo_model(self, model_name: str = None, model_type: str = "face") -> Optional[YOLO]:
        if model_name is None:
            model_name = MODEL_CONFIG["face_detection_model"] if model_type == "face" else MODEL_CONFIG[
                "person_detection_model"]

        cache_key = f"yolo_{model_name}_{model_type}"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]

        try:
            model_path = self.models_dir / model_name
            model_urls = {
                "yolov8n-face.pt": "https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov8n-face.pt",
                "yolov8s-face.pt": "https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov8s-face.pt",
                "yolov8m-face.pt": "https://github.com/akanametov/yolov8-face/releases/download/v0.0.0/yolov8m-face.pt",
                "yolov8n.pt": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt",
                "yolov8s.pt": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt",
                "yolov8m.pt": "https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt",
            }

            if not model_path.exists():
                if model_name in model_urls:
                    if not self._download_model(model_urls[model_name], model_path):
                        logger.info(f"âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ù…Ù† ÙÙŠ YOLO: {model_name}")
                        model = YOLO(model_name)
                else:
                    logger.info(f"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ù…Ù†")
                    model = YOLO(model_name)
            else:
                logger.info(f"ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠ: {model_name}")
                model = YOLO(str(model_path))

            model.to(self.device)
            model.model.float()
            if hasattr(model, 'model'):
                model.model.float()

            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆØªÙØ¹ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ YOLO ({model_name}) Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {self.device}")
            self.model_cache[cache_key] = model
            return model

        except Exception as e:
            logger.info(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ YOLO {model_name}: {e}")
            return None

    def load_whisper_model(self, model_name: str = None) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper"""
        if model_name is None:
            model_name = MODEL_CONFIG["speech_recognition_model"]

        cache_key = f"whisper_{model_name}"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]

        try:
            logger.info(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper: {model_name}")

            available_models = MODEL_CONFIG["available_whisper_models"]
            if model_name not in available_models:
                logger.info(f"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… 'base' Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†Ù‡")
                model_name = "base"

            model = whisper.load_model(
                model_name,
                download_root=str(self.models_dir / "whisper"),
                device=self.device
            )

            model = model.float()

            self.model_cache[cache_key] = model
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Whisper ({model_name}) Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {self.device}")
            return model

        except Exception as e:
            logger.info(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Whisper: {e}")
            return None

    def load_blip_model(self, model_name: str = "Salesforce/blip-image-captioning-base"):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BLIP Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ"""
        try:
            if model_name not in self.model_cache:
                processor = BlipProcessor.from_pretrained(model_name)
                # Force float32 for better compatibility
                model = BlipForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float32) # <--- CHANGE THIS LINE
                model.to(self.device)
                self.model_cache[model_name] = (processor, model)
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BLIP: {model_name}")
            return self.model_cache[model_name]
        except Exception as e:
            logger.info(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BLIP: {e}")
            return None, None

    def load_videomae_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ VideoMAE Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø· ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ."""
        model_key = "videomae"
        if model_key in self.model_cache:
            logger.info("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ VideoMAE ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©.")
            return self.model_cache[model_key]
        try:
            model_name = "MCG-NJU/videomae-base-finetuned-kinetics"
            logger.info(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ VideoMAE: {model_name}")
            processor = AutoFeatureExtractor.from_pretrained(model_name)
            # Force float32 for better compatibility
            model = VideoMAEForVideoClassification.from_pretrained(model_name, torch_dtype=torch.float32).to(self.device) # <--- CHANGE THIS LINE
            # Note: .to(self.device) should be after from_pretrained if you specify dtype
            self.model_cache[model_key] = (processor, model)
            logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ VideoMAE Ø¨Ù†Ø¬Ø§Ø­.")
            return processor, model
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ VideoMAE: {e}")
            return None, None


    def load_person_detector(self) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙƒØ´Ù Ø§Ù„Ø£Ø´Ø®Ø§Øµ"""
        return self.load_yolo_model(MODEL_CONFIG["person_detection_model"], "person")

    def load_face_detector(self) -> Optional[Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡"""
        return self.load_yolo_model(MODEL_CONFIG["face_detection_model"], "face")

    def clear_model_cache(self, model_key: str = None):
        """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        if model_key:
            if model_key in self.model_cache:
                # ØªÙØ±ÙŠØº Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                model_instance = self.model_cache[model_key]
                if isinstance(model_instance, tuple): # Ø¥Ø°Ø§ ÙƒØ§Ù† tuple (Ù…Ø«Ù„ BLIP: processor, model)
                    for item in model_instance:
                        del item
                else:
                    del model_instance
                del self.model_cache[model_key]
                logger.info(f"ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {model_key}")
        else:
            for key in list(self.model_cache.keys()):
                self.clear_model_cache(key)
            if GPU_AVAILABLE:
                torch.cuda.empty_cache()
            logger.info("ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©")

    def get_model_memory_usage(self) -> Dict[str, float]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
        memory_usage = {}

        for model_name, model in self.model_cache.items():
            try:
                if hasattr(model, 'parameters'):
                    # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
                    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                    total_size = (param_size + buffer_size) / 1024 ** 2  # MB

                    memory_usage[model_name] = total_size
            except:
                pass

        return memory_usage

    def load_scene_recognition_model(self):
        videomae_processor, videomae_model = self.load_videomae_model()
        blip_processor, blip_model = self.load_blip_model()
        return videomae_processor, videomae_model, blip_processor, blip_model

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.model_cache.clear()
        if GPU_AVAILABLE:
            torch.cuda.empty_cache()
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø±Ø¯ ModelLoader")


# Ø¥Ù†Ø´Ø§Ø¡ loader Ø¹Ø§Ù„Ù…ÙŠ
model_loader = ModelLoader()

def cleanup_models():
    """ØªÙ†Ø¸ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    model_loader.cleanup()

