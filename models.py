
import torch
import cv2
import numpy as np
import easyocr
from PIL import Image
from collections import defaultdict
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import logging
from collections import Counter
from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor
import types  # ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
from config import MODELS_DIR, PROCESSING_CONFIG, MODEL_CONFIG
from model_loader import model_loader
from activity_recognizer import ActivityRecognizer
from ultralytics import YOLO
from PIL import Image, ImageEnhance
from scrfd import SCRFD, Threshold
import supervision as sv

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logger = logging.getLogger(__name__)

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
device = model_loader.device


class FaceDetector:
    def __init__(self):
        self.scrfd_detector = SCRFD.from_path("scrfd.onnx", providers=["CUDAExecutionProvider"])

        if self.scrfd_detector is None:
            raise Exception("ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SCRFD Ù„ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡.")
        self.threshold = Threshold(probability=PROCESSING_CONFIG["face_detection_threshold"])
        self.frame_counter = 0
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]

        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª NMS (Ø¨Ø¯ÙˆÙ† Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø©)
        self.nms_threshold = 0.4  # Ø¹ØªØ¨Ø© NMS Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©

    def detect_faces(self, frame: np.ndarray, frame_number: int) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ÙÙŠ Ø¥Ø·Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø¨Ø§Ø´Ø±Ø© (Ø¨Ø¯ÙˆÙ† Ù†Ø§ÙØ°Ø© Ù…Ù†Ø²Ù„Ù‚Ø©)"""
        self.frame_counter += 1
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        if self.frame_counter % self.sampling_interval != 0:
            return []

        height, width, _ = frame.shape
        all_raw_detections = []  # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ù‚Ø¨Ù„ NMS

        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø·Ø§Ø± Ø¥Ù„Ù‰ PIL Image Ù„Ù„ÙƒØ´Ù Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
            pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

            # Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨Ø£ÙƒÙ…Ù„Ù‡
            faces_in_frame = self.scrfd_detector.detect(pil_frame, threshold=self.threshold)

            # Ø¬Ù…Ø¹ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª
            for face in faces_in_frame:
                x1 = int(face.bbox.upper_left.x)
                y1 = int(face.bbox.upper_left.y)
                x2 = int(face.bbox.lower_right.x)
                y2 = int(face.bbox.lower_right.y)

                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¶Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø¥Ø·Ø§Ø±
                x1 = max(0, min(x1, width))
                y1 = max(0, min(y1, height))
                x2 = max(x1, min(x2, width))
                y2 = max(y1, min(y2, height))

                all_raw_detections.append({
                    "bbox": [x1, y1, x2, y2],  # ØªÙ†Ø³ÙŠÙ‚ [x1, y1, x2, y2]
                    "confidence": face.probability,
                    "keypoints": face.keypoints  # SCRFD ÙŠØ¹ÙŠØ¯ keypoints Ù…Ø¨Ø§Ø´Ø±Ø©
                })

            # ØªØ·Ø¨ÙŠÙ‚ NMS Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©
            final_detections = self._apply_nms(all_raw_detections)

            faces = []
            for i, det in enumerate(final_detections):
                x1, y1, x2, y2 = det["bbox"]
                width_face = x2 - x1
                height_face = y2 - y1

                faces.append({
                    "frame_number": frame_number,
                    "bbox": [x1, y1, width_face, height_face],  # ØªÙ†Ø³ÙŠÙ‚ [x, y, w, h]
                    "confidence": det["confidence"],
                    "face_id": i,  # ÙÙ‡Ø±Ø³ ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ ÙˆØ¬Ù‡ Ø¨Ø¹Ø¯ NMS
                    "keypoints": {  # ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†
                        "left_eye": {"x": det["keypoints"].left_eye.x, "y": det["keypoints"].left_eye.y},
                        "right_eye": {"x": det["keypoints"].right_eye.x, "y": det["keypoints"].right_eye.y},
                        "nose": {"x": det["keypoints"].nose.x, "y": det["keypoints"].nose.y},
                        "left_mouth": {"x": det["keypoints"].left_mouth.x, "y": det["keypoints"].left_mouth.y},
                        "right_mouth": {"x": det["keypoints"].right_mouth.x, "y": det["keypoints"].right_mouth.y}
                    }
                })

            if faces:
                logger.info(f"ğŸ‘¥ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(faces)} ÙˆØ¬ÙˆÙ‡ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number} Ø¨Ø¹Ø¯ NMS.")

            return faces

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡: {e}")
            return []

    def _apply_nms(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ØªØ·Ø¨ÙŠÙ‚ Non-Maximum Suppression Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª"""
        if not detections:
            return []

        boxes = np.array([d["bbox"] for d in detections])  # [x1, y1, x2, y2]
        scores = np.array([d["confidence"] for d in detections])

        # cv2.dnn.NMSBoxes ØªØªÙˆÙ‚Ø¹ [x, y, w, h]ØŒ Ù„Ø°Ø§ Ù†Ø­ØªØ§Ø¬ Ù„Ù„ØªØ­ÙˆÙŠÙ„
        boxes_xywh = np.array([[x1, y1, x2 - x1, y2 - y1] for x1, y1, x2, y2 in boxes])

        indices = cv2.dnn.NMSBoxes(
            boxes_xywh.tolist(),
            scores.tolist(),
            score_threshold=float(self.threshold.probability),
            nms_threshold=self.nms_threshold
        )

        if isinstance(indices, np.ndarray):
            indices = indices.flatten().tolist()
        elif isinstance(indices, list) and len(indices) > 0 and isinstance(indices[0], list):
            indices = [i[0] for i in indices]
        else:
            indices = []  # ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§ÙƒØªØ´Ø§ÙØ§Øª Ø¨Ø¹Ø¯ NMS

        return [detections[i] for i in indices]

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        if hasattr(self, 'scrfd_detector'):
            self.scrfd_detector = None
            logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ FaceDetector")


class FrameEnhancer:
    def __init__(self, brightness=1.0, contrast=1.0, saturation=1.0):
        self.brightness = brightness
        self.contrast = contrast
        self.saturation = saturation

    def enhance_frame(self, frame):
        if frame is None or frame.size == 0:
            logger.warning("âš ï¸ ØªÙ… ØªÙ…Ø±ÙŠØ± Ø¥Ø·Ø§Ø± ÙØ§Ø±Øº Ø¥Ù„Ù‰ FrameEnhancer.enhance_frame.")
            return None
        # ØªØ­ÙˆÙŠÙ„ BGR Ø¥Ù„Ù‰ RGB
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # ØªØ­ÙˆÙŠÙ„ numpy array Ø¥Ù„Ù‰ PIL Image
        img_pil = Image.fromarray(img_rgb)
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø³Ø·ÙˆØ¹ ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ† ÙˆØ§Ù„ØªØ´Ø¨Ø¹
        if self.brightness != 1.0:
            img_pil = ImageEnhance.Brightness(img_pil).enhance(self.brightness)
        if self.contrast != 1.0:
            img_pil = ImageEnhance.Contrast(img_pil).enhance(self.contrast)
        if self.saturation != 1.0:
            img_pil = ImageEnhance.Color(img_pil).enhance(self.saturation)
        return img_pil


class TextDetector:
    def __init__(self):
        self.detection_threshold = PROCESSING_CONFIG["text_detection_threshold"]
        self.enabled = PROCESSING_CONFIG["text_detection_enabled"]
        self.min_text_confidence = PROCESSING_CONFIG["min_text_confidence"]
        self.languages = MODEL_CONFIG["easyocr_languages"]
        self.reader = None
        self.frame_counter = 0
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]

        if self.enabled:
            self._setup_easyocr()

    def _setup_easyocr(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ EasyOCR Ù…Ø¹ Ø§Ù„Ø¥ØµÙ„Ø§Ø­"""
        try:
            from config import EASYOCR_CONFIG
            logger.info("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ EasyOCR...")

            gpu = EASYOCR_CONFIG["gpu_enabled"] and torch.cuda.is_available()
            model_dir = Path(EASYOCR_CONFIG["model_storage_directory"])
            model_dir.mkdir(parents=True, exist_ok=True)

            # âœ… Ø§Ù„ØªØµØ­ÙŠØ­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
            self.reader = easyocr.Reader(
                lang_list=self.languages,
                gpu=gpu,
                model_storage_directory=str(model_dir),
                download_enabled=EASYOCR_CONFIG["download_enabled"],
                detector=EASYOCR_CONFIG["detector"],  # âœ… ØªØµØ­ÙŠØ­ Ø§Ø³Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø©
                recognizer=EASYOCR_CONFIG["recognizer"]  # âœ… ØªØµØ­ÙŠØ­ Ø§Ø³Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø©
            )

            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ EasyOCR Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {'GPU' if gpu else 'CPU'}")

        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ EasyOCR: {e}")
            self.reader = None
            self.enabled = False

    def detect_text(self, frame: np.ndarray, frame_number: int) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„Ù†Øµ ÙÙŠ Ø¥Ø·Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        if self.reader is None or not self.enabled:
            return []

        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙ‚Ø·
        self.frame_counter += 1
        if self.frame_counter % self.sampling_interval != 0:
            return []

        try:
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ
            enhanced_frame = self._enhance_image_for_text(frame)
            results = self.reader.readtext(enhanced_frame, paragraph=False)

            text_data = []
            for (bbox, text, confidence) in results:
                if confidence >= self.min_text_confidence:
                    points = np.array(bbox).astype(int)
                    x1, y1 = np.min(points[:, 0]), np.min(points[:, 1])
                    x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])
                    width, height = x2 - x1, y2 - y1

                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                    if width < 10 or height < 10:
                        continue

                    language = self._detect_language(text)
                    text_data.append({
                        "frame_number": int(frame_number),
                        "bbox": [int(x1), int(y1), int(width), int(height)],
                        "text": text.strip(),
                        "confidence": float(confidence),
                        "language": language
                    })

            if text_data:
                logger.info(f"ğŸ“ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(text_data)} Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}")

            return text_data

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ù†Øµ: {e}")
            return []

    def _enhance_image_for_text(self, frame: np.ndarray) -> np.ndarray:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
        if frame is None or frame.size == 0:
            logger.warning("âš ï¸ ØªÙ… ØªÙ…Ø±ÙŠØ± Ø¥Ø·Ø§Ø± ÙØ§Ø±Øº Ø¥Ù„Ù‰ FrameEnhancer.enhance_frame.")
            return None
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # ØªØ·Ø¨ÙŠÙ‚ ØªØµØ­ÙŠØ­ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)

            # ØªØ­ÙˆÙŠÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ BGR
            return cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)

        except Exception:
            return frame  # Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£

    def _detect_language(self, text: str) -> str:
        """ÙƒØ´Ù Ø§Ù„Ù„ØºØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
        arabic_chars = "Ø§Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠØ¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ"

        if any(char in arabic_chars for char in text):
            return "ar"
        elif any(char.isalpha() for char in text):
            return "en"
        return "unknown"

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© EasyOCR"""
        if self.reader:
            try:
                # ØªÙ†Ø¸ÙŠÙ Models Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.reader = None
                logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© EasyOCR")
            except Exception as e:
                logger.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ EasyOCR: {e}")


class SpeechRecognizer:
    def __init__(self):
        self.model_name = MODEL_CONFIG["speech_recognition_model"]
        self.device = device
        self.model = None
        self._load_model()

    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…"""
        try:
            logger.info(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ({self.model_name}) Ø¹Ù„Ù‰ {self.device}...")
            self.model = model_loader.load_whisper_model(self.model_name)

            if self.model is not None:
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¹Ù„Ù‰ {self.device}")
            else:
                logger.error("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {e}")
            self.model = None

    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ"""
        if self.model is None:
            return {"text": "", "language": "unknown", "confidence": 0.0}

        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ø§Ù„ØµÙˆØª
            if not Path(audio_path).exists():
                logger.error(f"âŒ Ù…Ù„Ù Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {audio_path}")
                return {"text": "", "language": "unknown", "confidence": 0.0}

            # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("ğŸµ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...")
            result = self.model.transcribe(audio_path, fp16=torch.cuda.is_available())

            # Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙÙŠ Ù…Ù„Ù
            try:
                text_output_path = Path(audio_path).parent / "transcription.txt"
                with open(text_output_path, 'w', encoding='utf-8') as f:
                    f.write(result["text"])
                logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ ÙÙŠ: {text_output_path}")
            except Exception as e:
                logger.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ: {e}")

            return {
                "text": result["text"],
                "language": result.get("language", "unknown"),
                "confidence": 0.9,  # Ù‚ÙŠÙ…Ø© ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
                "segments": result.get("segments", [])
            }

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {e}")
            return {"text": "", "language": "unknown", "confidence": 0.0}

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.model = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ SpeechRecognizer")


class ObjectTracker:
    def __init__(self):
        # 1. ØªØ¹Ø±ÙŠÙ self.device Ø£ÙˆÙ„Ø§Ù‹ (Ù‚Ø¨Ù„ Ø£ÙŠ ØªØ­Ù…ÙŠÙ„)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ØŒ Ù„ØªØ¬Ù†Ø¨ AttributeError Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„)
        self.model = None
        self.image_processor = None
        self.id2label = None
        self.frame_counter = 0  # <--- ØªØµØ­ÙŠØ­: ØªØ¹Ø±ÙŠÙ frame_counter Ù‡Ù†Ø§ Ù…Ø¨ÙƒØ±Ù‹Ø§ (Ù„ÙƒÙ† Ø³Ù†Ø¹ÙŠØ¯ ØªØ¹ÙŠÙŠÙ†Ù‡ Ø¥Ø°Ø§ Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„)
        self.tracker = None
        self.box_annotator = None
        self.label_annotator = None
        self.trace_annotator = None
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]
        self.min_track_length = PROCESSING_CONFIG.get("min_track_length", 5)
        self.detection_threshold = PROCESSING_CONFIG.get("object_detection_threshold", 0.3)
        self.confidence_threshold = PROCESSING_CONFIG.get("object_confidence_threshold", 0.5)

        # 3. Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        try:
            logger.info(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ RT-DETR v2 r18vd Ø¹Ù„Ù‰ {self.device}...")
            self.image_processor = RTDetrImageProcessor.from_pretrained("PekingU/rtdetr_v2_r18vd")
            self.model = RTDetrV2ForObjectDetection.from_pretrained("PekingU/rtdetr_v2_r18vd").to(self.device)
            self.model.eval()

            # 4. ØªØ¹ÙŠÙŠÙ† self.id2label Ø¨Ø¹Ø¯ Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„
            self.id2label = self.model.config.id2label

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† id2label ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "person" (ID 0 ÙÙŠ COCO)
            if 0 not in self.id2label or self.id2label[0] != "person":
                logger.warning("âš ï¸ ÙØ¦Ø© 'person' (ID 0) ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ id2label. Ù‚Ø¯ ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØªØ¨Ø¹.")

            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ RT-DETR v2 Ø¨Ù†Ø¬Ø§Ø­. Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {len(self.id2label)}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ RT-DETR v2: {e}")
            # 5. Ø±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ (ÙŠÙ…Ù†Ø¹ Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† ØºÙŠØ± ØµØ§Ù„Ø­)
            raise Exception(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ RT-DETR v2 r18vd Ù„Ù„ØªØªØ¨Ø¹: {e}")

        # 6. ØªÙ‡ÙŠØ¦Ø© Ù…ÙƒÙˆÙ†Ø§Øª supervision ÙÙ‚Ø· Ø¥Ø°Ø§ Ù†Ø¬Ø­ Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ù‡Ù†Ø§ ÙŠØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©)
        try:
            self.tracker = sv.ByteTrack()
            self.box_annotator = sv.BoxAnnotator()
            self.label_annotator = sv.LabelAnnotator()
            self.trace_annotator = sv.TraceAnnotator()
            self.frame_counter = 0  # <--- Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† frame_counter Ù‡Ù†Ø§ Ù„Ù„ØªØ£ÙƒÙŠØ¯ (Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ø¬Ø§Ø­)

            logger.info(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© ObjectTracker (ByteTrack + RT-DETR v2) Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù…ÙƒÙˆÙ†Ø§Øª supervision: {e}")
            raise Exception(f"ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© ObjectTracker: {e}")

    def track_objects(self, frame: np.ndarray, frame_number: int) -> Tuple[sv.Detections, List[Dict[str, Any]]]:
        """
        ÙŠÙƒØ´Ù ÙˆÙŠØªØªØ¨Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ø£Ø´Ø®Ø§Øµ) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RT-DETR v2 r18vd Ùˆ ByteTrack.
        ÙŠØ¹ÙŠØ¯ ÙƒØ§Ø¦Ù† sv.Detections ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØªØ¨Ø¹ Ù„Ù„Ø£Ø´Ø®Ø§Øµ.
        """
        all_detections = []
        person_tracks = []

        # 7. ØªØ­Ù‚Ù‚ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù€ frame_counter (Ù„Ù„Ù…ØªØ§Ù†Ø©ØŒ ÙÙŠ Ø­Ø§Ù„Ø© Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©)
        if not hasattr(self, 'frame_counter'):
            logger.error("âŒ ObjectTracker ØºÙŠØ± Ù…ÙÙ‡ÙŠØ£ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­: frame_counter ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return sv.Detections.empty(), []

        self.frame_counter += 1

        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        if self.frame_counter % self.sampling_interval != 0:
            return sv.Detections.empty(), []

        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ­Ù…Ù„
            if self.model is None or self.image_processor is None:
                logger.error("âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙØ­Ù…Ù„. ØªØ£ÙƒØ¯ Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©.")
                return sv.Detections.empty(), []

            h, w, _ = frame.shape
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            inputs = self.image_processor(images=rgb_frame, return_tensors="pt").to(self.device)

            with torch.no_grad():
                with torch.autocast(device_type="cuda" if self.device.type == "cuda" else "cpu",
                                    dtype=torch.float16 if self.device.type == "cuda" else torch.float32):
                    outputs = self.model(**inputs)

            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† post_process_object_detection
            processed_results = self.image_processor.post_process_object_detection(
                outputs,
                target_sizes=torch.tensor([(h, w)], device=self.device),
                threshold=self.detection_threshold
            )[0]

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§ÙƒØªØ´Ø§ÙØ§Øª (boxes, scores, labels)
            if 'boxes' not in processed_results or len(processed_results['boxes']) == 0:
                return sv.Detections.empty(), []

            # Ø¥Ù†Ø´Ø§Ø¡ sv.Detections Ù…Ù† Ù†ØªØ§Ø¦Ø¬ transformers
            detections = sv.Detections.from_transformers(
                transformers_results=processed_results,
                id2label=self.id2label
            )

            # ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø«Ù‚Ø©
            if detections.confidence is not None:
                mask = detections.confidence > self.confidence_threshold
                detections = detections[mask]

            if len(detections) == 0:
                return sv.Detections.empty(), []

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØªØ¨Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ByteTrack
            detections = self.tracker.update_with_detections(detections)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            for i in range(len(detections)):
                if detections.xyxy is None or i >= len(detections.xyxy):
                    continue  # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ØµØ§Ù„Ø­Ø©

                bbox = detections.xyxy[i].tolist()  # [x1, y1, x2, y2]
                if len(bbox) != 4:
                    logger.warning(f"âš ï¸ bbox ØºÙŠØ± ØµØ§Ù„Ø­ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}: {bbox}. ØªØ®Ø·ÙŠ.")
                    continue

                conf = detections.confidence[i] if detections.confidence is not None else 0.0
                cls_id = detections.class_id[i] if detections.class_id is not None else 0
                x1, y1, x2, y2 = map(int, bbox)
                class_name = self.id2label.get(int(cls_id), f"unknown_{cls_id}")
                track_id = detections.tracker_id[i] if detections.tracker_id is not None else None

                all_detections.append({
                    "bbox": bbox,
                    "confidence": conf,
                    "class_name": class_name,
                    "frame_number": frame_number,
                    "track_id": track_id if class_name == "person" else None
                })

                if class_name == "person" and track_id is not None:
                    person_tracks.append({
                        "track_id": track_id,
                        "frame_number": frame_number,
                        "bbox": [x1, y1, x2, y2],
                        "confidence": conf,
                        "class_name": class_name,
                    })

            if len(detections) > 0:
                logger.info(
                    f"ğŸ”„ ØªÙ… ÙƒØ´Ù ÙˆØªØªØ¨Ø¹ {len(detections)} ÙƒØ§Ø¦Ù† ({len(person_tracks)} Ø´Ø®Øµ) ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}")

            return detections, person_tracks

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù ÙˆØªØªØ¨Ø¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª (ByteTrack): {e}")
            import traceback
            logger.error(f"ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø®Ø·Ø£: {traceback.format_exc()}")  # Ù„Ù„ØªØ´Ø®ÙŠØµ
            return sv.Detections.empty(), []

    def draw_tracks(self, frame: np.ndarray, detections: sv.Detections) -> np.ndarray:
        """Ø±Ø³Ù… Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø±"""
        if len(detections) == 0 or detections.xyxy is None:
            return frame

        # Ø¨Ù†Ø§Ø¡ labels Ù…Ø¹ ØªØ­Ù‚Ù‚
        labels = []
        for cls_id, tid, conf in zip(detections.class_id, detections.tracker_id, detections.confidence):
            if tid is not None and cls_id is not None and conf is not None:
                class_name = self.id2label.get(int(cls_id), 'unknown')
                labels.append(f"#{int(tid)} {class_name} {conf:.2f}")
            else:
                labels.append("unknown")

        # Ø±Ø³Ù… Ø§Ù„ØµÙ†Ø§Ø¯ÙŠÙ‚
        annotated_frame = self.box_annotator.annotate(frame.copy(), detections=detections)
        # Ø±Ø³Ù… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
        annotated_frame = self.label_annotator.annotate(annotated_frame, detections=detections, labels=labels)
        # Ø±Ø³Ù… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
        annotated_frame = self.trace_annotator.annotate(annotated_frame, detections=detections)

        return annotated_frame

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            self.model = None
            self.image_processor = None
            self.tracker = None
            self.box_annotator = None
            self.label_annotator = None
            self.trace_annotator = None
            self.id2label = None
            logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ ObjectTracker (ByteTrack + RT-DETR)")
        except Exception as e:
            logger.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ ObjectTracker: {e}")



