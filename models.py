import torch
import cv2
import numpy as np
import easyocr

from collections import defaultdict
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import logging

from config import MODELS_DIR, PROCESSING_CONFIG, MODEL_CONFIG
from model_loader import model_loader
from person_detector import PersonDetector
from ultralytics import YOLO
from PIL import Image, ImageEnhance
from basicsr.archs.rrdbnet_arch import RRDBNet
from realesrgan import RealESRGANer

logger = logging.getLogger(__name__)
device = model_loader.device


class FaceDetector:
    def __init__(self):
        self.model = model_loader.load_yolo_model(MODEL_CONFIG["face_detection_model"], "face")
        self.threshold = PROCESSING_CONFIG["face_detection_threshold"]
        self.min_face_size = PROCESSING_CONFIG["min_face_size"]
        self.max_face_size = PROCESSING_CONFIG["max_face_size"]
        self.aspect_ratio_min = PROCESSING_CONFIG["face_aspect_ratio_min"]
        self.aspect_ratio_max = PROCESSING_CONFIG["face_aspect_ratio_max"]
        self.frame_counter = 0
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]
        self.class_names = ['face']

    def detect_faces(self, frame: np.ndarray, frame_number: int) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ÙÙŠ Ø¥Ø·Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ù…Ø¹ Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙˆØ§Ù„ØªØµÙÙŠØ©"""
        if self.model is None:
            return []

        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙ‚Ø·
        self.frame_counter += 1
        if self.frame_counter % self.sampling_interval != 0:
            return []

        try:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.model(frame_rgb, conf=self.threshold, verbose=False)

            faces = []
            for result in results:
                if hasattr(result, 'boxes') and result.boxes is not None:
                    for i, box in enumerate(result.boxes):
                        confidence = float(box.conf[0])
                        if confidence >= self.threshold:
                            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())

                            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø±Ø¨Ø¹ Ø¶Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø©
                            x1 = max(0, x1)
                            y1 = max(0, y1)
                            x2 = min(frame.shape[1], x2)
                            y2 = min(frame.shape[0], y2)

                            width = x2 - x1
                            height = y2 - y1

                            # ØªØµÙÙŠØ© Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… ÙˆÙ†Ø³Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¶ Ø¥Ù„Ù‰ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹
                            if self._is_valid_face(width, height):
                                faces.append({
                                    "frame_number": frame_number,
                                    "bbox": [x1, y1, width, height],
                                    "confidence": confidence,
                                    "face_id": i
                                })

            if faces:
                logger.info(f"ğŸ‘¥ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(faces)} ÙˆØ¬ÙˆÙ‡ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}")

            return faces

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡: {e}")
            return []

    def _is_valid_face(self, width: int, height: int) -> bool:
        """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ÙƒØ§Ø¦Ù† Ø§Ù„Ù…ÙƒØªØ´Ù Ù‡Ùˆ ÙˆØ¬Ù‡ Ø­Ù‚ÙŠÙ‚ÙŠ"""
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ù…
        if width < self.min_face_size or height < self.min_face_size:
            return False

        if width > self.max_face_size or height > self.max_face_size:
            return False

        if height > 0:
            aspect_ratio = width / height
            if aspect_ratio < self.aspect_ratio_min or aspect_ratio > self.aspect_ratio_max:
                return False

        return True

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        if hasattr(self, 'model'):
            self.model = None
            logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ FaceDetector")

class FrameEnhancer:
    def __init__(self, model_path='RealESRGAN_x4plus.pth', device=None, brightness=1.0, contrast=1.0,
                         saturation=1.0):
        self.brightness = brightness
        self.contrast = contrast
        self.saturation = saturation
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        self.half = self.device.type == 'cuda'
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
        self.upsampler = RealESRGANer(
                    scale=4,
                    model_path=model_path,
                    model=model,
                    tile=512,
                    tile_pad=128,
                    pre_pad=0,
                    half=self.half,
                    device=self.device
                )
    def enhance_frame(self, frame):
        # ØªØ­ÙˆÙŠÙ„ BGR Ø¥Ù„Ù‰ RGB
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        '''with torch.no_grad():
            output_rgb, _ = self.upsampler.enhance(img_rgb, outscale=4)'''
        # ØªØ­ÙˆÙŠÙ„ numpy array Ø¥Ù„Ù‰ PIL Image
        img_pil = Image.fromarray(img_rgb)
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø³Ø·ÙˆØ¹ ÙˆØ§Ù„ØªØ¨Ø§ÙŠÙ† ÙˆØ§Ù„ØªØ´Ø¨Ø¹
        if self.brightness != 1.0:
            img_pil = ImageEnhance.Brightness(img_pil).enhance(self.brightness)
        if self.contrast != 1.0:
            img_pil = ImageEnhance.Contrast(img_pil).enhance(self.contrast)
        if self.saturation != 1.0:
            img_pil = ImageEnhance.Color(img_pil).enhance(self.saturation)
        return img_pil


class TextDetector:
    def __init__(self):
        self.detection_threshold = PROCESSING_CONFIG["text_detection_threshold"]
        self.enabled = PROCESSING_CONFIG["text_detection_enabled"]
        self.min_text_confidence = PROCESSING_CONFIG["min_text_confidence"]
        self.languages = MODEL_CONFIG["easyocr_languages"]
        self.reader = None
        self.frame_counter = 0
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]

        if self.enabled:
            self._setup_easyocr()

    def _setup_easyocr(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ EasyOCR Ù…Ø¹ Ø§Ù„Ø¥ØµÙ„Ø§Ø­"""
        try:
            from config import EASYOCR_CONFIG
            logger.info("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ EasyOCR...")

            gpu = EASYOCR_CONFIG["gpu_enabled"] and torch.cuda.is_available()
            model_dir = Path(EASYOCR_CONFIG["model_storage_directory"])
            model_dir.mkdir(parents=True, exist_ok=True)

            # âœ… Ø§Ù„ØªØµØ­ÙŠØ­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
            self.reader = easyocr.Reader(
                lang_list=self.languages,
                gpu=gpu,
                model_storage_directory=str(model_dir),
                download_enabled=EASYOCR_CONFIG["download_enabled"],
                detector=EASYOCR_CONFIG["detector"],
                recognizer=EASYOCR_CONFIG["recognizer"]
            )

            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ EasyOCR Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {'GPU' if gpu else 'CPU'}")

        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ EasyOCR: {e}")
            self.reader = None
            self.enabled = False

    def detect_text(self, frame: np.ndarray, frame_number: int) -> List[Dict[str, Any]]:
        """ÙƒØ´Ù Ø§Ù„Ù†Øµ ÙÙŠ Ø¥Ø·Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        if self.reader is None or not self.enabled:
            return []

        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª ÙÙ‚Ø·
        self.frame_counter += 1
        if self.frame_counter % self.sampling_interval != 0:
            return []

        try:
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ
            enhanced_frame = self._enhance_image_for_text(frame)
            results = self.reader.readtext(enhanced_frame, paragraph=False)

            text_data = []
            for (bbox, text, confidence) in results:
                if confidence >= self.min_text_confidence:
                    points = np.array(bbox).astype(int)
                    x1, y1 = np.min(points[:, 0]), np.min(points[:, 1])
                    x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])
                    width, height = x2 - x1, y2 - y1

                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                    if width < 10 or height < 10:
                        continue

                    language = self._detect_language(text)
                    text_data.append({
                        "frame_number": int(frame_number),
                        "bbox": [int(x1), int(y1), int(width), int(height)],
                        "text": text.strip(),
                        "confidence": float(confidence),
                        "language": language
                    })

            if text_data:
                logger.info(f"ğŸ“ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {len(text_data)} Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame_number}")

            return text_data

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ù Ø§Ù„Ù†Øµ: {e}")
            return []

    def _enhance_image_for_text(self, frame: np.ndarray) -> np.ndarray:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # ØªØ·Ø¨ÙŠÙ‚ ØªØµØ­ÙŠØ­ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            enhanced = clahe.apply(gray)

            # ØªØ­ÙˆÙŠÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ BGR
            return cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)

        except Exception:
            return frame  # Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£

    def _detect_language(self, text: str) -> str:
        """ÙƒØ´Ù Ø§Ù„Ù„ØºØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"""
        arabic_chars = "Ø§Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠØ¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ"

        if any(char in arabic_chars for char in text):
            return "ar"
        elif any(char.isalpha() for char in text):
            return "en"
        return "unknown"

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© EasyOCR"""
        if self.reader:
            try:
                # ØªÙ†Ø¸ÙŠÙ Models Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                self.reader = None
                logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© EasyOCR")
            except Exception as e:
                logger.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ EasyOCR: {e}")


class SpeechRecognizer:
    def __init__(self):
        self.model_name = MODEL_CONFIG["speech_recognition_model"]
        self.device = device
        self.model = None
        self._load_model()

    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…"""
        try:
            logger.info(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ({self.model_name}) Ø¹Ù„Ù‰ {self.device}...")
            self.model = model_loader.load_whisper_model(self.model_name)

            if self.model is not None:
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¹Ù„Ù‰ {self.device}")
            else:
                logger.error("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {e}")
            self.model = None

    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ"""
        if self.model is None:
            return {"text": "", "language": "unknown", "confidence": 0.0}

        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù Ø§Ù„ØµÙˆØª
            if not Path(audio_path).exists():
                logger.error(f"âŒ Ù…Ù„Ù Ø§Ù„ØµÙˆØª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {audio_path}")
                return {"text": "", "language": "unknown", "confidence": 0.0}

            # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© GPU Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("ğŸµ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...")
            result = self.model.transcribe(audio_path, fp16=torch.cuda.is_available())

            # Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙÙŠ Ù…Ù„Ù
            try:
                text_output_path = Path(audio_path).parent / "transcription.txt"
                with open(text_output_path, 'w', encoding='utf-8') as f:
                    f.write(result["text"])
                logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ ÙÙŠ: {text_output_path}")
            except Exception as e:
                logger.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Øµ Ø§Ù„ØµÙˆØªÙŠ: {e}")

            return {
                "text": result["text"],
                "language": result.get("language", "unknown"),
                "confidence": 0.9,  # Ù‚ÙŠÙ…Ø© ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
                "segments": result.get("segments", [])
            }

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {e}")
            return {"text": "", "language": "unknown", "confidence": 0.0}

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.model = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ SpeechRecognizer")



class ObjectTracker:
    def __init__(self):
        self.person_detector = PersonDetector()
        self.tracking_threshold = PROCESSING_CONFIG["tracking_threshold"]
        self.max_tracking_distance = PROCESSING_CONFIG["max_tracking_distance"]
        self.min_track_length = PROCESSING_CONFIG["min_track_length"]
        self.tracks = defaultdict(list)
        self.next_track_id = 1
        self.colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0),
            (0, 0, 128), (128, 128, 0), (128, 0, 128), (0, 128, 128)
        ]

    def track_objects(self, frame: np.ndarray, persons: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª"""
        tracking_data = []
        for person in persons:
            bbox_list = person["bbox"]
            center_x = bbox_list[0] + bbox_list[2] / 2
            center_y = bbox_list[1] + bbox_list[3] / 2
            min_distance = float('inf')
            best_track_id = None
            for track_id, track in self.tracks.items():
                if track:
                    last_pos = track[-1]
                    distance = ((center_x - last_pos[0]) ** 2 + (center_y - last_pos[1]) ** 2) ** 0.5
                    if distance < min_distance and distance < self.max_tracking_distance:
                        min_distance = distance
                        best_track_id = track_id

            if best_track_id is not None:
                track_id = best_track_id
                self.tracks[track_id].append((center_x, center_y))
            else:
                track_id = self.next_track_id
                self.next_track_id += 1
                self.tracks[track_id] = [(center_x, center_y)]
            tracking_data.append({
                "track_id": track_id,
                "frame_number": person["frame_number"],
                "bbox": bbox_list.tolist(),
                "confidence": person["confidence"],
                "type": "person"
            })
        if tracking_data:
                logger.info(f"ğŸ”„ ØªÙ… ØªØªØ¨Ø¹ {len(tracking_data)} Ø£Ø´Ø®Ø§Øµ ÙÙŠ Ø§Ù„Ø¥Ø·Ø§Ø± {frame.shape[0]}") # Ø§Ø³ØªØ®Ø¯Ù… frame.shape[0] Ø£Ùˆ Ø£ÙŠ Ù…Ø¤Ø´Ø± Ù„Ù„Ø¥Ø·Ø§Ø±

        return tracking_data

    def draw_tracks(self, frame: np.ndarray, tracking_data: List[Dict[str, Any]]) -> np.ndarray:
        """Ø±Ø³Ù… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø±"""
        for track in tracking_data:
            track_id = track["track_id"]
            bbox = track["bbox"]
            color = self.colors[track_id % len(self.colors)]

            x1, y1, x2, y2 = map(int, bbox)

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

            if track_id in self.tracks and len(self.tracks[track_id]) >= self.min_track_length:
                path = self.tracks[track_id]
                for i in range(1, len(path)):
                    cv2.line(frame,
                             (int(path[i - 1][0]), int(path[i - 1][1])),
                             (int(path[i][0]), int(path[i][1])),
                             color, 2)

                cv2.putText(frame, f"Person {track_id}", (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return frame

    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ØªØ¹Ù‚Ø¨"""
        self.tracks.clear()
        self.next_track_id = 1
        logger.info("ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ØªØ¹Ù‚Ø¨")

    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.tracks.clear()
        if hasattr(self, 'person_detector'):
            self.person_detector.cleanup()
            self.person_detector = None
        logger.info("ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ Ù…ÙˆØ§Ø±Ø¯ ObjectTracker")

class GeneralObjectDetector:
    def __init__(self, model_name="yolov8n.pt", device="cuda"):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model = YOLO(model_name)
        self.model.to(self.device)
        self.frame_counter = 0
        self.model.fuse()  # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.threshold = PROCESSING_CONFIG["person_detection_threshold"]
        self.sampling_interval = PROCESSING_CONFIG["frame_sampling_interval"]
    def detect_objects(self, frame, frame_number):

        self.frame_counter += 1
        if self.frame_counter % self.sampling_interval != 0:
            return []
        results = self.model(frame)
        detections = []
        for result in results:
            boxes = result.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
            confidences = result.boxes.conf.cpu().numpy()
            class_ids = result.boxes.cls.cpu().numpy().astype(int)
            class_names = [self.model.names[cid] for cid in class_ids]
            for box, confidence, cname in zip(boxes, confidences, class_names):
                detections.append({
                    "bbox": box,
                    "confidence": confidence,
                    "class_name": cname,
                    "frame_number" : frame_number
                })
        return detections


